<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Noon van der Silk - The Ethics of AI - An Empathy-Based Approach?</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
        <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/javascript">
          MathJax.Hub.Config({
            tex2jax: {
              inlineMath: [['$','$'], ['\\(','\\)']],
              processEscapes: true
            }
          });
        </script>
        <script>
        var host = "silky.github.io";
        if ((host == window.location.host) && (window.location.protocol != "https:")) {
          window.location.protocol = "https";
        }
        </script>
        <link href="https://fonts.googleapis.com/css?family=Taviraj|PT+Mono" rel="stylesheet">
        </head>
    <body>
        <div id="header">
            <div id="logo">
                <h1><a href="../">Noon van der Silk</a></h1>
            </div>
            <div id="navigation">
                <a href="../about.html">About</a>
                <a href="../archive.html">Archive</a>
                <a href="https://betweenbooks.com.au/">Between Books</a>
                <a href="../talks.html">Talks</a>
                <a href="../links.html">Links</a>
                <small><a href="../atom.xml">Atom Feed</a></small>
            </div>
        </div>

        <div id="content">
            <h2>The Ethics of AI - An Empathy-Based Approach?</h2>

            <div class="info">
    Posted on July 25, 2018
    
        by Noon van der Silk
    
</div>

<p>There’s lots of talk about the Ethics of AI at the moment. As with any
research, there’s too much for any one person to read. Here’s a bunch of
papers that I’ve collected haphazardly in the early part of this year:</p>
<ul>
<li><a href="https://scirate.com/arxiv/1711.03846">“Dave…I can assure you…that it’s going to be all right…” – A definition, case for, and survey of algorithmic assurances in human-autonomy trust relationships</a></li>
<li><a href="https://scirate.com/arxiv/1709.06692">A Voting-Based System for Ethical Decision Making</a></li>
<li><a href="https://scirate.com/arxiv/1511.06578">Actually, It’s About Ethics in Computational Social Science: A Multi-party Risk-Benefit Framework for Online Community Research</a></li>
<li><a href="https://scirate.com/arxiv/1711.07373">Attentive Explanations: Justifying Decisions and Pointing to the Evidence (Extended Abstract)</a></li>
<li><a href="https://scirate.com/arxiv/1502.05838">Automated Reasoning for Robot Ethics</a></li>
<li><a href="https://scirate.com/arxiv/1702.00137">Blue Sky Ideas in Artificial Intelligence Education from the EAAI 2017 New and Future AI Educator Program</a></li>
<li><a href="https://scirate.com/arxiv/1710.06881">Children and the Data Cycle: Rights and Ethics in a Big Data World</a></li>
<li><a href="https://scirate.com/arxiv/1606.06565">Concrete Problems in AI Safety</a></li>
<li><a href="https://scirate.com/arxiv/1711.07076">Does mitigating ML’s impact disparity require treatment disparity?</a></li>
<li><a href="https://scirate.com/arxiv/1706.03021">Ethical Artificial Intelligence - An Open Question</a></li>
<li><a href="https://scirate.com/arxiv/1701.07769">Ethical Considerations in Artificial Intelligence Courses</a></li>
<li><a href="https://scirate.com/arxiv/1707.05259">Ethics of autonomous information systems towards an artificial thinking</a></li>
<li><a href="https://scirate.com/arxiv/1504.05603">Formalizing Preference Utilitarianism in Physical World Models</a></li>
<li><a href="https://scirate.com/arxiv/1703.06354">Goal Conflict in Designing an Autonomous Artificial System</a></li>
<li><a href="https://scirate.com/arxiv/1610.03229">In The Wild Residual Data Research and Privacy</a></li>
<li><a href="https://scirate.com/arxiv/1709.05929">Institutionally Distributed Deep Learning Networks</a></li>
<li><a href="https://scirate.com/arxiv/1711.05791">Maintaining The Humanity of Our Models</a></li>
<li><a href="https://scirate.com/arxiv/1607.08289">Mammalian Value Systems</a></li>
<li><a href="https://scirate.com/arxiv/1710.06882">Mapping for accessibility: A case study of ethics in data science for social good</a></li>
<li><a href="https://scirate.com/arxiv/1711.07111">Modeling Epistemological Principles for Bias Mitigation in AI Systems: An Illustration in Hiring Decisions</a></li>
<li><a href="https://scirate.com/arxiv/1711.06664">Predict Responsibly: Increasing Fairness by Learning To Defer</a></li>
<li><a href="https://scirate.com/arxiv/1609.03266">Recovering the History of Informed Consent for Data Science and Internet Industry Research Ethics</a></li>
<li><a href="https://scirate.com/arxiv/1706.02513">Responsible Autonomy</a></li>
<li><a href="https://scirate.com/arxiv/1701.02388">Stoic Ethics for Artificial Agents</a></li>
<li><a href="https://scirate.com/arxiv/1606.02583">The Dark Side of Ethical Robots</a></li>
<li><a href="https://scirate.com/arxiv/1711.00561">This robot stinks! Differences between perceived mistreatment of robot and computer partners</a></li>
<li><a href="https://scirate.com/arxiv/1703.04741">Towards Moral Autonomous Systems</a></li>
<li><a href="https://scirate.com/arxiv/1711.05905">Using experimental game theory to transit human values to ethical AI</a></li>
</ul>
<p>One thing I wanted to think about is, speaking as someone working in this
field and interested in making changes in my day-to-day life, what kind of
tools or ideas would be useful for me? What should I do?</p>
<p>Alongside this thought, another thought I had is that somehow the big lists
of rules feel very impersonal and disconnected from my experiences. I also feel
a little bit unsatisfied about opt-in rules. Here’s a few from the around the
place, that I’ve seen:</p>
<ul>
<li><a href="https://futureoflife.org/ai-principles/">Future of Life</a> (June 2018, relevant items)
<ul>
<li>5 - Race Avoidance: Teams developing AI systems should actively cooperate to
avoid corner-cutting on safety standards.</li>
<li>6 - Safety: AI systems should be safe and secure throughout their
operational lifetime, and verifably so where applicable and feasible</li>
<li>7 - Failure Transparency: If an AI system causes harm, it should be
possible to ascertain why.</li>
<li>8 - Judical Transparency: Any involvement by an autonomous system in
judicial decision-making should provide a satisfactory explanation
auditable by a competent human authority.</li>
<li>9 - Responsibility: Designers and building of advanced AI systems are
stakeholders in the moral implications of their use, misuse, and actions,
with a responsibility and opportunity to shape those implications.</li>
<li>10 - Value Alignment: Highly autonomous AI systems should be designed so
that their goals and behviours</li>
<li>11 - Human Values: AI Systems should be designed and operated so as to
be compatible with ideals of human dignift, rights, freedoms, and cultural
diversity.</li>
<li>12 - Personal Privacy: People should have the right to access, manage
and control the data they generate, given AI systems’ power to analyze and
utilize that data.</li>
<li>13 - Liberty and Privacy: The application of AI to personal data must
not unreasonably curtail people’s real or perceived liberty.</li>
<li>14 - Shared Benefit: AI technologies should benefit and empower as many
people as possible.</li>
<li>15 - Shared Prosperity: The economic prosperity created by AI should be
shared broadly, to benefit all of humanity.</li>
<li>16 - Human Control: Humans should choose how and whether to delegate
decisions to AI systems, to accomplish human chose objectives.</li>
<li>17 - Non-subversion: The power conferred by control of highly advanced
AI systems should respect and improve, rather an subvert, the social and
civic processes on which the health of society depends.</li>
<li>18 - AI Arms Race: An arms race in lethal autonomous weapons should be
avoided.</li>
</ul></li>
<li><a href="https://www.aiforhumanity.fr/en/">AI For Humanity</a> (June 2018)
<ul>
<li>01 - Develop an aggressive data policy</li>
<li>02 - Targeting four strategic sectors</li>
<li>03 - Boosting the potential of French research</li>
<li>04 - Planning for the impact of AI on labour</li>
<li>05 - Making AI more environmentally friendly</li>
<li>06 - Opening up the black boxes of AI</li>
<li>07 - Ensuring that AI supports inclusivity and diversity</li>
</ul></li>
<li><a href="http://humansforai.com/">Humans for AI</a> (June 2018)
<ul>
<li>Broaden the pipeline of minorities currently in tech careers, seeking to
move to careers in AI by being the go to destination for all things AI
because we believe that diversity of thought and opinion ultimately builds
better products.</li>
<li>Open and inclusive community of people interested in AI by facilitating
interactions with experts, practitioners and thought leaders in the field.</li>
<li>Leverage AI to release a set of free products built by this community to
further our mission of bringing diversity to AI.</li>
<li>Demystify AI by providing a basic understanding of the concepts,
thinking and events in AI for novices and non-technical people interested
in how AI will impact their lives and their jobs.</li>
</ul></li>
<li><a href="https://arxiv.org/pdf/1606.06565.pdf">Concrete Problems in AI Safety</a> (2016)
<ul>
<li>Avoid Negative Side Effects</li>
<li>Avoid Reward Hacking</li>
<li>Scalable Oversight</li>
<li>Safe Exploration</li>
<li>Robustness to Distributional Shift</li>
</ul></li>
</ul>
<p>I have a few problems with these rules:</p>
<ul>
<li>It’s easy to imagine situations in which they are counter-productive,</li>
<li>I don’t feel a lot of ownership of them, as I wasn’t involved in their construction,</li>
<li>No-one is enforcing them on me,</li>
<li>They’re often highly impractical or contain colloquial/regional/policital concerns (“Boost French Research …”),</li>
<li>They’re also very overwhelming and demanding, how can I ensure that we do <em>all</em> of them?</li>
<li>Even if I <em>say</em> I’m doing these things, how does any non-technical person know? How can I prove it?</li>
</ul>
<p>The positive aspects of them are:</p>
<ul>
<li>It’s sometimes easy to think about how to apply them to day-to-day work,</li>
<li>They help me think of things that I might not care about day-to-day (i.e. the environmental concerns?),</li>
<li>It might help to lobby governments/organisations to get funding to make progress on certain aspects?</li>
<li>It provides a framework that might be useful for discussing with colleagues/other people</li>
</ul>
<p>So, what should any given engineer working in this area do? One thought I’ve
had recently is a simple one: Let’s just aim at building empathy for the
people that will be affected by our software.</p>
<p>This is reasonably actionable, say, with local groups by organising meetings
between technical people and the people that may be affected. I.e. in the
medical-AI setting, let’s organise regular catch-ups between the engineers,
the doctors, nursing staff, and hospital adminstration types, along with
perhaps patient representatives.</p>
<p>In the setting, of, say, law software, again we just set up regular events for
the two groups to chat through issues, work together on small projects, and
build a mutual understanding of difficulties.</p>
<p>I think this approach is a bit nicer than, say, creating a new set of rules
that make sense for us locally, and then forcing people to follow them. One
idea I like about the empathy-based/collaborative approach (or “human-centered
design”; another term for this kind thing), is that it allos people to <em>adapt
to local circumstances</em>, which I think is really crucial in allowing any one
person to feel like they have some control over the application of any rules
they come up with, and thus getting them to actually take an interest in
enforcing them in their organisation.</p>
<p>So, my new rule of thumb for this ethics-related AI stuff will be: Can I meet
with some of the people that will be affected? What are their thoughts? What
problems are they working through and what are they interested in?</p>
<p>As always, I’m interested in your thoughts on the matter!</p>

        </div>
        <div id="footer">
            <small><a href="http://jaspervdj.be/hakyll">Hakyll</a> was involved here (<a href="https://github.com/silky/silky.github.com">source</a>).
                ن
            </small>
        </div>
        <div id="image_footer"> &nbsp; </div>
    </body>
</html>
