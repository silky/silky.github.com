<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Noon van der Silk - Quantum neural networks</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
        <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/javascript">
          MathJax.Hub.Config({
            tex2jax: {
              inlineMath: [['$','$'], ['\\(','\\)']],
              processEscapes: true
            }
          });
        </script>
        <script>
        var host = "silky.github.io";
        if ((host == window.location.host) && (window.location.protocol != "https:")) {
          window.location.protocol = "https";
        }
        </script>
        <link href="https://fonts.googleapis.com/css?family=Taviraj|PT+Mono" rel="stylesheet">
        </head>
    <body>
        <div id="header">
            <div id="logo">
                <h1><a href="../">Noon van der Silk</a></h1>
            </div>
            <div id="navigation">
                <a href="../about.html">About</a>
                <a href="../archive.html">Archive</a>
                <a href="../library.html">Library</a>
                <a href="../talks.html">Talks</a>
                <a href="../links.html">Links</a>
                <small><a href="../atom.xml">Atom Feed</a></small>
            </div>
        </div>

        <div id="content">
            <h2>Quantum neural networks</h2>

            <div class="info">
    Posted on December 11, 2016
    
        by Noon van der Silk
    
</div>

<p>(This post requires a background in the basics of quantum computing (and neural networks). Please have a read of the first part of <a href="../posts/2014-09-09-intro-to-qc-and-the-surface-code.html">Introduction to quantum computing and the surface code</a> if you’d like to get up to speed on the quantum parts, <a href="http://neuralnetworksanddeeplearning.com/">Neural networks and Deep Learning</a> is a good introduction to the other part.)</p>
<p>Recently, I’ve been spending a lot of time thinking about machine learning, and in particular deep learning. But before that, I was mostly concerning myself with quantum computing, and specifically the algorithmic/theory side of quantum computing.</p>
<p>In the last few days there’s been a flurry of papers on quantum machine learning/quantum neural networks, and related topics. Infact, there’s been a fair bit of research in the last few years (see the <a href="#appendix">Appendix</a> at the end for a few links), and I thought I’d take this opportunity to have a look at what people are up to.</p>
<p>The papers we’ll be discussing are:</p>
<ul>
<li><a href="https://scirate.com/arxiv/1612.01045">arXiv:1612.01045 - Quantum generalisation of feedforward neural networks</a> by Wan, Dahlsten, Kristjánsson, Gardner and Kim.</li>
<li><a href="https://scirate.com/arxiv/1612.01789">arXiv:1612.01789 - Quantum gradient descent and Newton’s method for constrained polynomial optimization</a> by Rebentrost, Schuld, Petruccione and Lloyd,</li>
<li><a href="https://scirate.com/arxiv/1612.02806">arXiv:1612.02806 - Quantum autoencoders for efficient compression of quantum data</a> by Romero, Olson and Aspuru-Guzik,</li>
</ul>
<p>But first, let’s take a look at the paper that got me interested in machine learning in the first place!</p>
<h2 id="arxiv1307.0411---quantum-algorithms-for-supervised-and-unsupervised-machine-learning">arXiv:1307.0411 - Quantum algorithms for supervised and unsupervised machine learning</h2>
<p>The paper, <a href="https://scirate.com/arxiv/1307.0411">Quantum algorithms for supervised and unsupervised machine learning</a> by Lloyd, Mohseni and Rebentrost in 2013, was one of my first technical exposures to machine learning. It’s an interesting one because it demonstrates that for certain types of clustering algorithms there is a quantum algorithm that exhibits an exponential speed-up over the classical counterpart.</p>
<p><strong>Aside</strong>: Gaining complexity-theoretic speed-ups is the central task of (quantum) complexity theory. The speedup in this paper is interested, but it “only” demonstrates a speed-up on a problem that is already known to be <em>efficient</em><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> for classical computers, so it <em>doesn’t</em> provide evidence that quantum computers are fundamentally more powerful than classical ones, by the standard notions in computer science.</p>
<p>The <code>Supervised clustering</code> problem that is tackled in the paper is as follows:</p>
<blockquote>
<p>Given some vector <span class="math inline"><em>u⃗</em> ∈ ℝ<sup><em>N</em></sup></span>, and two sets of vectors <span class="math inline"><em>V</em></span> and <span class="math inline"><em>W</em></span>, then given <span class="math inline"><em>M</em></span> representative samples from <span class="math inline"><em>V</em></span>: <span class="math inline"><em>v⃗</em><sub><em>j</em></sub> ∈ <em>V</em></span> and <span class="math inline"><em>w⃗</em><sub><em>k</em></sub> ∈ <em>W</em></span>, figure out which set <span class="math inline"><em>u⃗</em></span> should go in to, by comparing the distances to these vectors.</p>
</blockquote>
<p>In pictures it looks like so:</p>
<div style="text-align: center">
<figure>
<img src="../images/supervised-clustering.png" alt="Figure 1. An example of the supervised clustering problem with \vec{u} \in \mathbb{R}^2 and M=3. We’ve drawn 3 points from V, the (unknown) dashed purple region, and 3 points from W, the dashed pink region." /><figcaption>Figure 1. An example of the supervised clustering problem with <span class="math inline"><em>u⃗</em> ∈ ℝ<sup>2</sup></span> and <span class="math inline"><em>M</em> = 3</span>. We’ve drawn 3 points from <span class="math inline"><em>V</em></span>, the (unknown) dashed purple region, and 3 points from <span class="math inline"><em>W</em></span>, the dashed pink region.</figcaption>
</figure>
</div>
<p>Classically, if we think about where we’d like to put <span class="math inline"><em>u⃗</em></span>, we could compare the distance to all the points <span class="math inline">$\vec{v_1}, \vec{v_2}, \vec{v_3}$</span> and to all the points <span class="math inline">$\vec{w_1}, \vec{w_2},\vec{w_3}$</span>. In the specific example I’ve drawn, doing so will show that, out of the two sets, <span class="math inline"><em>u⃗</em></span> belongs in <span class="math inline"><em>V</em></span>.</p>
<p>In general, we can see that, using this approach, we would need to look at all <span class="math inline"><em>M</em></span> data points, and we’d need to compare each dimension of the <span class="math inline"><em>N</em></span> dimensions of each vector <span class="math inline">$\vec{v_j}, \vec{w_k}, \vec{u}$</span>; i.e. we’d need to look at at least <span class="math inline"><em>M</em> × <em>N</em></span> pieces of information. In other words, we’d compute the distance</p>
<p></p>
<p>By looking at the problem slightly more formally, we find that classically the best known algorithm takes “something like”<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> <span class="math inline"><code>pol</code><em>y</em>(<em>M</em> × <em>N</em>)</span> steps, where “<span class="math inline"><code>poly</code></span>” means that the true running time is a polynomial of <span class="math inline"><em>M</em> × <em>N</em></span>.</p>
<p>Quantumly, the paper demonstrates an algorithm that lets us solve this problem in “something like” <span class="math inline">log (<em>M</em> × <em>N</em>)</span> time steps. This is a significant improvement, in a practical sense. To get an idea, if we took <span class="math inline">100</span> samples from a <span class="math inline"><em>N</em> = 350</span>-dimensional space, then <span class="math inline"><em>M</em> × <em>N</em> = 35, 000</span> and <span class="math inline">log (<em>M</em> × <em>N</em>) ≈ 10</span>.</p>
<p>The quantum algorithm works by constructing a state in a certain state so that, when measured, the distance that we wanted, <span class="math inline"><em>d</em>(<em>u⃗</em>, <em>V</em>)</span>, is the probability that we achieve a certain measurement outcome. In this way, we can build this certain state, and measure it, several times, and use this information to approximate the required distances. And, the paper shows that this whole process can be done in “something like” a running time of <span class="math inline">log (<em>M</em> × <em>N</em>)</span>.</p>
<p>There’s more contirbutions in the paper than just this; so it’s worth a look if you’re interested.</p>
<h2 id="arxiv16.12.01045---quantum-generalisation-of-feedforward-neural-networks">arXiv:16.12.01045 - Quantum generalisation of feedforward neural networks</h2>
<p>So this paper is pretty cool. We can get a feel for what it’s doing by first considering the following network:</p>
<div style="text-align: center">
<figure>
<img src="../images/basic-nn.png" alt="Figure 2. A simple classical neural network" /><figcaption>Figure 2. A simple classical neural network</figcaption>
</figure>
</div>
<p>This network has two inputs, <span class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub></span>, three learnable weights, <span class="math inline"><em>w</em><sub>1</sub>, <em>w</em><sub>2</sub>, <em>w</em><sub>3</sub></span>, one output value <span class="math inline"><em>y</em></span>, and an activation function <span class="math inline"><em>f</em></span>.</p>
<p>Classically one would feed in a series of training examples <span class="math inline">(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, <em>y</em>)</span> and update the weights according to some loss function to achieve the best result for the given data.</p>
<p>Quantumly, there are some immediate problems with doing this, if we switch the inputs <span class="math inline"><em>x</em></span> to be quantum states, instead of classical real variables.</p>
<p>The problems are:</p>
<ul>
<li>Quantum operations need to be reversible; <span class="math inline"><em>f</em></span> as written is not (at the very least, it takes two inputs and squashes them down to one output).</li>
<li>Quantum states need to be normalised; so multiplying them by a single arbitrary weight won’t be productive.</li>
<li>You can’t copy arbitrary quantum states due to the <a href="https://en.wikipedia.org/wiki/No-cloning_theorem">No-Cloning theorem</a>, so this restricts the type of networks that can be written down.</li>
</ul>
<p>The way this paper solves these problems is to transition Figure 2 from a classical non-reversible network to a reversible quantum one:</p>
<div style="text-align: center">
<figure>
<img src="../images/transition-to-quantum-nn.png" alt="Figure 3. The transition from classical, to reversible classical, to quantum." /><figcaption>Figure 3. The transition from classical, to reversible classical, to quantum.</figcaption>
</figure>
</div>
<p>The final network takes in an arbitrary quantum state of two qubits, <span class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub></span>, and then adjoins an ancilla state <span class="math inline">|0⟩</span>, applies some unitary operation <span class="math inline"><em>U</em></span>, and emits a combined final state <span class="math inline">|<em>ψ</em>⟩<sub><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, <em>y</em></sub><sup>Out</sup></span> where the final qubit <span class="math inline"><em>y</em></span> contains the result we’re interested in.</p>
<p>At this point, one might reasonably ask: How is this different to a quantum circuit? It appears to me that the only difference is that <span class="math inline"><em>U</em></span> is actually unknown, and <em>it</em> is trainable! Note that this is also a somewhat radical difference from classical neural networks: there, we don’t normally think of the activation functions (defined as <span class="math inline"><em>f</em></span> above) as trainable parameters; but quantumly, in this paper, that’s exactly how we think of them!</p>
<p>It turns out that unitary matrices can be parameterised by a collection of real variables <span class="math inline"><em>α</em></span>. Consider an arbitrary unitary matrix operating on two qubits, then <span class="math inline"><em>U</em></span> can be written as:</p>
<p></p>
<p>where <span class="math inline"><em>σ</em><sub><em>i</em></sub>, <em>i</em> ∈ 1, 2, 3</span> are the usual <a href="https://en.wikipedia.org/wiki/Pauli_matrices">Pauli matrices</a> and <span class="math inline"><em>σ</em><sub>0</sub></span> is the <span class="math inline">2 × 2</span> identity matrix. So one can then make these parameters <span class="math inline"><em>α</em><sub><em>j</em><sub>1</sub>, <em>j</em><sub>2</sub></sub></span> the <em>trainable</em> parameters! <s>It turns out that in the paper they don’t train these parameters explicitly, instead they pick a less general way of writing down unitary matricies, and they construct, by hand, a unitary for two qubits. It’s not clear why they’ve done this, and it would not be fun to have to build a special trainable unitary matrix for each node/neuron of your architecture depending on its input.</s></p>
<p><strong>Update:</strong> Kwok-Ho kindly corrected me that they <em>do</em> indeed train directly on this form of unitary matricies, and that the simplification they do in the paper is used to investigate the loss surface.</p>
<p>In any case, the main contribution of this paper seems to me to be the idea that we can <em>learn</em> unitary matricies for our particular problem. They go on to demonstrate that this idea works to build a quantum autoencoder, and to make a neural network discover unitary matricies that perform the quantum teleportation protocol.</p>
<p>One view is that trying to learn arbitrary unitary matrices that perform a task really well will become too hard as the number neurons grows. If we had a large network, with potentially millions of internal, neurons (and hence unitaries) to learn, then it might be more effective to fix unitaries and instead focus on learning the weights.</p>
<p>However, it’s a promising technique that would be fun to try out.</p>
<h2 id="arxiv1612.01789---quantum-gradient-descent-and-newtons-method-for-constrained-polynomial-optimization">arXiv:1612.01789 - Quantum gradient descent and Newton’s method for constrained polynomial optimization</h2>
<p>Those of you familiar with neural networks will know that the central idea used to train them is <a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent</a>. We recall that gradient descent lets us known how to modify some vector <span class="math inline"><em>x</em></span> so that it does “better” when evaluated with some cost function <span class="math inline"><em>C</em>(<em>x</em>, <em>y</em>)</span> where <span class="math inline"><em>y</em></span> is some known good answer. I.e. <span class="math inline"><em>x</em></span> might be a probability of liking some object, and <span class="math inline"><em>y</em></span> might be the true probability, and <span class="math inline"><em>C</em>(<em>x</em>, <em>y</em>) = |<em>x</em> − <em>y</em>|<sup>2</sup></span>.</p>
<p>The paper supposes we have some quantum state <span class="math inline">$|x\rangle = \sum_{j=1}^N x_j |j\rangle$</span> (where <span class="math inline">|<em>j</em>⟩</span> is the <span class="math inline"><em>j</em></span>’th computational-basis state), and some cost function <span class="math inline"><em>C</em>(|<em>x</em>⟩, |<em>y</em>⟩)</span> that tells us how good <span class="math inline">|<em>x</em>⟩</span> is. The question is, given we can evaluate <span class="math inline"><em>C</em>(|<em>x</em>⟩, |<em>y</em>⟩)</span>, how can we best work out to modify <span class="math inline">|<em>x</em>⟩</span> to do better?</p>
<p>If this was entirely classical, we could just calculate the gradient of <span class="math inline"><em>C</em></span> with respect to the variables <span class="math inline"><em>x</em><sub><em>j</em></sub></span>, and then propose a new set of <span class="math inline"><em>x</em><sub><em>j</em></sub></span>’s. However, we can’t inspect all these values quantumly, so we need to do something else.</p>
<p>In the paper, they demonstrate an approach that requires a few copies of the current state <span class="math inline">|<em>x</em><sup>(<em>t</em>)</sup>⟩</span>, but will produce a new state <span class="math inline">|<em>x</em><sup>(<em>t</em> + 1)</sup>⟩</span> such that (with objective/loss function <span class="math inline"><em>f</em></span>):</p>
<p></p>
<p>for some step size <span class="math inline"><em>η</em></span>. That is, it’s a step in the (hopefully) right direction, as per normal gradient descent!</p>
<p>So one direction to take this paper would be to build a “fully quantum” neural network like so:</p>
<div style="text-align: center">
<figure>
<img src="../images/fully-quantum-nn.png" alt="Figure 5. Fully-quantum neural network. The inputs and the weights are quantum states; and the activation function U is unitary." /><figcaption>Figure 5. Fully-quantum neural network. The inputs and the weights are quantum states; and the activation function <span class="math inline"><em>U</em></span> is unitary.</figcaption>
</figure>
</div>
<p>where we make the weights quantum states, <s>and the weights are multiplied onto the inputs as a dot-product</s>. This would require that the weight state is the same size as the input state; but that should be possible because we’re the ones building the network structure.</p>
<p><strong>Update</strong>: The idea about multiplying weights in didn’t make any sense; a much more sensible idea would be to prepare something like <span class="math inline">|<em>w</em><sub><em>i</em></sub>⟩⟨<em>w</em><sub><em>i</em></sub>|</span> and enact this on the input <span class="math inline">|<em>x</em><sub><em>k</em></sub></span> and <em>then</em> apply the fixed unitary.</p>
<p>We could then not worry about learning unitary matrices, and analogously to standard neural networks, just pick some unitary <span class="math inline"><em>U</em></span> that “works well” in practice, maybe by just defining quantum analogues of some common activation functions, perhaps say the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> or <a href="https://arxiv.org/abs/1511.07289">ELU</a>.</p>
<p>Overall I think that the quantum gradient descent algorithm should be useful for training neural networks, and maybe some cool things will come from it. There are some natural direct extensions of this work; namely to extend the implementation to <a href="http://sebastianruder.com/optimizing-gradient-descent/index.html">the more practical variations</a>.</p>
<h2 id="quantum-autoencoders-for-efficient-compression-of-quantum-data">1612.02806 - Quantum autoencoders for efficient compression of quantum data</h2>
<p>This paper came out only a few days after the Wan et al paper that we covered above, that also discussed autoencoders, so I thought it was worth a glance to see if this team did things differently.</p>
<p>This paper again takes the approach of not concerning itself with weights and instead focuses on learning a good unitary matrix <span class="math inline"><em>U</em></span> with a specific cost function.</p>
<p>They take a different approach in how they build their unitaries. Here they have a “programmable” quantum circuit, where they consider the parameters defining this circuit as the ones that can be trained. Given that these parameters are classical, and loss function they calculate is classical, no special optimisation techniques are needed.</p>
<h2 id="final-thoughts">Final thoughts</h2>
<p>It appears that the building blocks are being put together to start doing some serious work on quantum machine learning/quantum deep networks. <a href="http://web.physics.ucsb.edu/~martinisgroup/">Google</a> and <a href="http://blogs.microsoft.com/next/2016/11/20/microsoft-doubles-quantum-computing-bet/#sm.001umo7pp17ozfkwvl81eu0szc8pf">Microsoft</a> are already heavily investing in quantum computers, Google in particular has something it calls the <a href="https://plus.google.com/+QuantumAILab">“Quantum A.I. Lab”</a>, and there are even independent <a href="http://rigetti.com/">quantum computer manufacturing groups</a>.</p>
<p>It seems like there are lots of options on which way to direct efforts in the quantum ML world, and with these recent developments on quantum ML techniques, the time appears to be right to be getting into quantum deep learning!</p>
<h2 id="appendix">Appendix <a id="appendix"></a></h2>
<p>More interesting quantum machine learning papers:</p>
<ul>
<li><a href="https://scirate.com/arxiv/1611.09347">1611.09347 - Quantum Machine Learning</a> (most recent review)</li>
<li><a href="https://scirate.com/arxiv/1610.08251">1610.08251 - Quantum-enhanced machine learning</a></li>
<li><a href="https://scirate.com/arxiv/1605.05370">1605.05370 - Training A Quantum Optimizer</a></li>
<li><a href="https://scirate.com/arxiv/1603.08675">1603.08675 - Quantum Recommender Systems</a></li>
<li><a href="https://scirate.com/arxiv/1512.06069">1512.06069 - Demonstration of quantum advantage in machine learning</a></li>
<li><a href="https://scirate.com/arxiv/1512.02900">1512.02900 - Advances in quantum machine learning</a></li>
<li><a href="https://scirate.com/arxiv/1611.08104">1611.08104 - Quantum Enhanced Inference in Markov Logic Networks</a></li>
<li><a href="https://scirate.com/arxiv/1609.02542">1609.02542 - Quantum-assisted learning of graphical models with arbitrary pairwise connectivity</a></li>
<li><a href="https://scirate.com/arxiv/1609.06935">1609.06935 - Quantum Neural Machine Learning - Backpropagation and Dynamics</a></li>
<li><a href="https://scirate.com/arxiv/1606.02318">1606.02318 - Solving the Quantum Many-Body Problem with Artificial Neural Networks</a></li>
<li>even more: <a href="https://scirate.com/search?utf8=%E2%9C%93&amp;q=quantum+machine+learning">SciRate - quantum machine learning</a>!</li>
</ul>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Here <em>efficient</em> means that the problem is in the complexity class caled <span class="math inline"><strong>P</strong></span>. Problems that are efficient for quantum computers are in the complexity class <span class="math inline"><strong>BQP</strong></span>. One of the main outstanding questions in the field is “Are quantum computers more powerful than classical ones?” and this can be phrased as comparing the class <span class="math inline"><strong>P</strong></span> and <span class="math inline"><strong>BQP</strong></span>.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p><em>“Something like” x</em> here is a very informal term for the more formal statement that the running time is <span class="math inline"><em>O</em>(<em>x</em>)</span>. See <a href="https://en.wikipedia.org/wiki/Big_O_notation">Big O Notation</a> for more.<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</section>

        </div>
        <div id="footer">
            <small><a href="http://jaspervdj.be/hakyll">Hakyll</a> was involved here (<a href="https://github.com/silky/silky.github.com">source</a>).
                <a href="https://braneshop.com.au/?bs=">Braneshop</a>. ن
            </small> 
        </div>
        <div id="image_footer"> &nbsp; </div>
    </body>
</html>
